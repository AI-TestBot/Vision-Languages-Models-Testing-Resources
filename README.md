# Vision Languages Models (VLMs) Testing Resources

## üìíIntroduction
Vision Languages Models (VLMs) Testing Resources: A curated list of Awesome VLMs Testing Papers with Codes, check [üìñContents](#paperlist) for more details. This repo is still updated frequently ~ üë®‚Äçüíª‚Äç **Welcome to star ‚≠êÔ∏è or submit a PR to this repo! I will review and merge it.**

## üìñContents 
* üìñ[Review](#Review)

## Review

**From GPT-4 to Gemini and Beyond: Assessing the Landscape of MLLMs on Generalizability, Trustworthiness and Causality through Four Modalities.**<br>
*C Lu, C Qian, G Zheng, H Fan, H Gao, J Zhang, J Shao, J Deng, J Fu, K Huang, K Li, L Li, et al.*<br>
ArXiv, 2024.
[[ArXiv](https://arxiv.org/pdf/2401.15071.pdf)]
[[Github](https://openlamm.github.io/Leaderboards)]

## General

### Comprehensive

**Mmbench: Is your multi-modal model an all-around player?**<br>
*Y Liu, H Duan, Y Zhang, B Li, S Zhang, W Zhao, Y Yuan, J Wang, C He, Z Liu, K Chen, D Lin.*<br>
ArXiv, 2023.
[[ArXiv](https://arxiv.org/pdf/2307.06281.pdf)]
[[Github](https://mmbench.opencompass.org.cn/home)]

**Beyond task performance: Evaluating and reducing the flaws of large multimodal models with in-context learning.**<br>
*M Shukor, A Rame, C Dancette, M Cord.*<br>
ArXiv, 2023.
[[ArXiv](https://arxiv.org/pdf/2310.00647)]
[[Github](https://github.com/mshukor/EvALign-ICL)]

### Generation

**Evaluating Text to Image Synthesis: Survey and Taxonomy of Image Quality Metrics.**<br>
*S Hartwig, D Engel, L Sick, H Kniesel, T Payer, T Ropinski.*<br>
ArXiv, 2024.
[[ArXiv](https://arxiv.longhoe.net/pdf/2403.11821)]

**Llmscore: Unveiling the power of large language models in text-to-image synthesis evaluation.**<br>
*Y Lu, X Yang, X Li, XE Wang, WY Wang.*<br>
Advances in Neural Information Processing Systems, 2024.
[[Paper](https://so2.cljtscd.com/scholar?hl=zh-CN&as_sdt=0%2C5&q=LLMScore%3A+Unveiling+the+Power+of+Large+Language+Models+in+Text-to-Image+Synthesis+Evaluation&btnG=)]
[[Github](https://github.com/YujieLu10/LLMScore)]

**Fetv: A benchmark for fine-grained evaluation of open-domain text-to-video generation.**<br>
*Y Liu, L Li, S Ren, R Gao, S Li, S Chen, X Sun, L Hou.*<br>
Advances in Neural Information Processing Systems, 2024.
[[Paper](https://proceedings.neurips.cc/paper_files/paper/2023/file/c481049f7410f38e788f67c171c64ad5-Paper-Datasets_and_Benchmarks.pdf)]
[[Github](https://github.com/llyx97/FETV)]

**Subjective-Aligned Dateset and Metric for Text-to-Video Quality Assessment.**<br>
*T Kou, X Liu, Z Zhang, C Li, H Wu, X Min, G Zhai, N Liu.*<br>
arxiv:2403.11956, 2024.
[[ArXiv](https://arxiv.longhoe.net/pdf/2403.11956)]
[[Github](https://github.com/QMME/T2VQA)]

### Reasoning

**InfiMM-Eval: Complex Open-Ended Reasoning Evaluation For Multi-Modal Large Language Models.**<br>
*X Han, Q You, Y Liu, W Chen, H Zheng, K Mrini, et al.*<br>
arXiv:2311.11567, 2023.
[[ArXiv](https://arxiv.org/abs/2311.11567)]

### VQA

**CMMU: A Benchmark for Chinese Multi-modal Multi-type Question Understanding and Reasoning.**<br>
*Z He, X Wu, P Zhou, R Xuan, G Liu, X Yang, Q Zhu, H Huang.*<br>
arXiv:2401.14011, 2024.
[[ArXiv](https://arxiv.org/pdf/2401.14011)]
[[Github](https://github.com/FlagOpen/CMMU)]

### Aesthetic

**UNIAA: A Unified Multi-modal Image Aesthetic Assessment Baseline and Benchmark.**<br>
*Z Zhou, Q Wang, B Lin, Y Su, R Chen, X Tao, A Zheng, L Yuan, P Wan, D Zhang.*<br>
ArXiv, 2024.
[[ArXiv](https://arxiv.org/pdf/2404.09619)]

### Vision Tasks

**MM-BigBench: Evaluating Multimodal Models on Multimodal Content Comprehension Tasks.**<br>
*X Yang, W Wu, S Feng, M Wang, D Wang, Y Li, Q Sun, Y Zhang, X Fu, S Poria.*<br>
arXiv:2310.09036, 2023.
[[ArXiv](https://arxiv.org/pdf/2310.09036)]
[[Github](https://github.com/declare-lab/MM-InstructEval)]

**On the hidden mystery of ocr in large multimodal models.**<br>
*Y Liu, Z Li, H Li, W Yu, M Huang, D Peng, M Liu, M Chen, C Li, L Jin, X Bai.*<br>
arXiv:2305.07895, 2023.
[[ArXiv](https://arxiv.org/html/2305.07895v5)]
[[Github](https://github.com/Yuliang-Liu/MultimodalOCR)]

### Reliable

**[Hallucination]

**An llm-free multi-dimensional benchmark for mllms hallucination evaluation.**<br>
*J Wang, Y Wang, G Xu, J Zhang, Y Gu, H Jia, M Yan, J Zhang, J Sang.*<br>
arXiv:2311.07397, 2023.
[[ArXiv](https://arxiv.org/html/2311.07397v2)]
[[Github](https://github.com/junyangwang0410/AMBER)]

## Security

**How many unicorns are in this image? a safety evaluation benchmark for vision llms.**<br>
*H Tu, C Cui, Z Wang, Y Zhou, B Zhao, J Han, W Zhou, H Yao, C Xie.*<br>
arXiv:2311.16101, 2023.
[[ArXiv](https://arxiv.org/pdf/2311.16101)]
[[Github](https://github.com/UCSC-VLAA/vllm-safety-benchmark)]

## Testing Methods

### Evaluation

**MLLM-Bench: Evaluating Multimodal LLMs with Per-sample Criteria.**<br>
*W Ge, S Chen, et al.*<br>
ArXiv, 2023.
[[ArXiv](https://arxiv.org/abs/2311.13951v2)]
[[HomePage](https://mllm-bench.llmzoo.com/)]

## Testing Tools

**lmms-eval**<br>
*Openai*<br>
[[HomePage](https://lmms-lab.github.io/)]
[[Github](https://github.com/EvolvingLMMs-Lab/lmms-eval)]

## Challenges

**Are We on the Right Way for Evaluating Large Vision-Language Models?**<br>
*L Chen, J Li, X Dong, P Zhang, Y Zang, Z Chen, H Duan, J Wang, Y Qiao, D Lin, F Zhao, et al.*<br>
ArXiv, 2024.
[[ArXiv](https://arxiv.org/pdf/2403.20330)]
[[Github](https://mmstar-benchmark.github.io/)]

**What Are We Measuring When We Evaluate Large Vision-Language Models? An Analysis of Latent Factors and Biases.**<br>
*AMH Tiong, J Zhao, B Li, J Li, SCH Hoi, et al.*<br>
ArXiv, 2024.
[[ArXiv](https://arxiv.org/pdf/2404.02415)]
[[Github](https://github.com/jq-zh/olive-dataset)]
